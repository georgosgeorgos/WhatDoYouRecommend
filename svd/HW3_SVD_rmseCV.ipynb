{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import stem\n",
    "import sklearn.metrics\n",
    "from random import randint\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "with open('../CollaborativeFiltering/new_userDict.json', 'r') as fp:\n",
    "    \n",
    "    new_userDict = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../CollaborativeFiltering/new_isbnDict.json', 'r') as fp:\n",
    "    \n",
    "    new_isbnDict = json.load(fp)\n",
    "\n",
    "    \n",
    "    \n",
    "with open('../CollaborativeFiltering/dict_row.json', 'r') as fp:\n",
    "    \n",
    "    dict_row = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../CollaborativeFiltering/dict_col.json', 'r') as fp:\n",
    "    \n",
    "    dict_col = json.load(fp)\n",
    "    \n",
    "        \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "with open('../CollaborativeFiltering/clustering/CLUSTERS_ITEMS.json', 'r') as fp:\n",
    "    \n",
    "    CLUSTERS_ITEMS = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../CollaborativeFiltering/clustering/CLUSTERS_USERS.json', 'r') as fp:\n",
    "    \n",
    "    CLUSTERS_USERS = json.load(fp)\n",
    "\n",
    "    \n",
    "with open('../CollaborativeFiltering/clustering/file/clusters_dict_row.json', 'r') as fp:\n",
    "    \n",
    "    clusters_dict_row = json.load(fp)\n",
    "       \n",
    "with open('../CollaborativeFiltering/clustering/file/clusters_dict_col.json', 'r') as fp:\n",
    "    \n",
    "    clusters_dict_col = json.load(fp)\n",
    "    \n",
    "print(\"Ok\") \n",
    "\n",
    "with open('../CollaborativeFiltering/clustering/index_book_user_clusters/index_user_cluster.json', 'r') as fp:\n",
    "    \n",
    "    index_user_cluster = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('../CollaborativeFiltering/clustering/index_book_user_clusters/index_book_cluster.json', 'r') as fp:\n",
    "    \n",
    "    index_book_cluster = json.load(fp)\n",
    "    \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSampleDict(new_isbnDict, new_userDict, t1, t2):\n",
    "    \n",
    "\n",
    "    small_isbnDict = {}\n",
    "\n",
    "    for book in new_isbnDict:\n",
    "\n",
    "        temp = new_isbnDict[book]\n",
    "        i = 0\n",
    "\n",
    "        for t in temp.values():\n",
    "            if t != \"0\":\n",
    "                i = i+1\n",
    "\n",
    "        if i >t1:\n",
    "\n",
    "            small_isbnDict[book] = new_isbnDict[book]   \n",
    "\n",
    "    small_userDict = {}\n",
    "\n",
    "    for user in new_userDict:\n",
    "\n",
    "        temp = new_userDict[user]\n",
    "        i = 0\n",
    "\n",
    "        for t in temp.values():\n",
    "            if t != \"0\":\n",
    "                i = i+1\n",
    "\n",
    "        if i >t2:\n",
    "\n",
    "            small_userDict[user] = new_userDict[user]\n",
    "            \n",
    "            \n",
    "    return small_isbnDict, small_userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeMatrices(train_userDict,train_isbnDict,small_userDict,small_isbnDict, dict_row, dict_col):\n",
    "\n",
    "    n = len(small_isbnDict)\n",
    "    m = len(small_userDict)\n",
    "    \n",
    "    index = sorted(small_userDict.keys())\n",
    "    columns = sorted(small_isbnDict.keys())\n",
    "\n",
    "    dict_row = {k:v for v,k in enumerate(index)}\n",
    "    dict_col = {k:v for v,k in enumerate(columns)}\n",
    "\n",
    "    u = np.zeros((m,n)) \n",
    "    R = np.zeros((m,n))\n",
    "    for user in train_userDict:\n",
    "        for isbn in train_userDict[user]:\n",
    "            try:\n",
    "                u[dict_row[user]][dict_col[isbn]] = train_userDict[user][isbn]\n",
    "                R[dict_row[user]][dict_col[isbn]] = 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    for isbn in train_isbnDict:\n",
    "        for user in train_isbnDict[isbn]:\n",
    "            try:\n",
    "                u[dict_row[user]][dict_col[isbn]] = train_isbnDict[isbn][user]\n",
    "                R[dict_row[user]][dict_col[isbn]] = 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    small_utility_DataFrame = pd.DataFrame(u, index = index, columns = columns)\n",
    "    #R = pd.DataFrame(R, index = index, columns = columns)\n",
    "\n",
    "    return u, R, small_utility_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_cluster, R_cluster, utility_DataFrame_cluster = computeMatrices(CLUSTERS_USERS,CLUSTERS_ITEMS,\n",
    "                                                                           CLUSTERS_USERS,CLUSTERS_ITEMS,\n",
    "                                                                            clusters_dict_row,clusters_dict_col)\n",
    "\n",
    "\n",
    "u, R, utility_DataFrame = computeMatrices(new_userDict,new_isbnDict,new_userDict,new_isbnDict,dict_row,dict_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize values X and theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalizeUtility(U, R):\n",
    "\n",
    "    U_i_mean = np.zeros((U.shape[0]))\n",
    "    U_j_mean = np.zeros((U.shape[1]))\n",
    "\n",
    "    for i in range(U.shape[0]):\n",
    "\n",
    "        index = np.where(R[i,:]==1)[0]\n",
    "\n",
    "        U_i_mean[i] = U[i,index].mean()\n",
    "        U[i,index] = U[i,index] - U[i,index].mean()\n",
    "        \n",
    "        \n",
    "    for j in range(U.shape[1]):\n",
    "\n",
    "        index = np.where(R[:,j]==1)[0]\n",
    "\n",
    "        U_j_mean[j] = U[index,j].mean()\n",
    "        U[index,j] = U[index,j] - U[index,j].mean()\n",
    "        \n",
    "    U = np.nan_to_num(U)\n",
    "    U_i_mean = np.nan_to_num(U_i_mean)\n",
    "    U_j_mean = np.nan_to_num(U_j_mean)\n",
    "        \n",
    "    return U, U_i_mean, U_j_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularized cost function\n",
    "\n",
    "$$ J = \\sum_{i=0}^{n}\\sum_{j=0}^{m}[(\\sum_{k=1}^{k} X_{ik}\\theta_{kj}) - U_{ij}]^2  + \\lambda \\sum_{i=0}^{n}\\sum_{k=0}^{k}X_{ik}^2  +  \\lambda \\sum_{j=0}^{m}\\sum_{k=0}^{k}\\theta_{kj}^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularized gradient\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial J}{\\partial X_{ik}}  =  \\sum_{j=0}^{m}\\{[(\\sum_{k=1}^{k} X_{ik}\\theta_{kj}) - U_{ij}]\\theta_{kj}\\} + \\lambda X_{ik} $$\n",
    "\n",
    "$$\\dfrac{\\partial J}{\\partial \\theta_{kj}}  =  \\sum_{i=0}^{n}\\{[(\\sum_{k=1}^{k} X_{ik}\\theta_{kj}) - U_{ij}]X_{ik}\\} + \\lambda \\theta_{kj} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradFunction(c,X,theta,alpha):\n",
    "    \n",
    "    grad_X = np.dot(c,theta.T) + alpha*X\n",
    "    grad_theta = np.dot(X.T,c) + alpha*theta\n",
    "    \n",
    "    return grad_X, grad_theta\n",
    "\n",
    "\n",
    "def costFunction(parameters,U,R,t,alpha):\n",
    "    \n",
    "    n, m = U.shape\n",
    "    \n",
    "    X = np.reshape(parameters[:n*t], (n,t))\n",
    "    theta = np.reshape(parameters[n*t:], (t,m))\n",
    "    \n",
    "    c = np.dot(X,theta)\n",
    "    c = c*R - U \n",
    "    C = (c**2).sum()\n",
    "    J = C + alpha*((X**2).sum()) + alpha*((theta**2).sum())\n",
    "    \n",
    "    grad_X, grad_theta = gradFunction(c,X,theta,alpha)\n",
    "    grad = np.concatenate((np.ravel(grad_X), np.ravel(grad_theta)))\n",
    "    \n",
    "    return J, grad  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Xtheta(parameters,U,t):\n",
    "    \n",
    "    n, m = U.shape\n",
    "    \n",
    "    X = np.reshape(parameters[:n*t], (n,t))\n",
    "    theta = np.reshape(parameters[n*t:], (t,m))\n",
    "    \n",
    "    return X,theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prediction(x, U, t, U_i_mean, U_j_mean):\n",
    "    \n",
    "    X_new,theta_new = Xtheta(x,U,t)\n",
    "    U_new = np.dot(X_new,theta_new)\n",
    "    \n",
    "    for row in range(U_new.shape[0]):\n",
    "        \n",
    "        U_new[row,:] = U_new[row,:] + U_i_mean[row]\n",
    "        \n",
    "    for col in range(U_new.shape[1]):\n",
    "        \n",
    "        U_new[:,col] = U_new[:,col] + U_j_mean[col]\n",
    "        \n",
    "    U_new[U_new<0] = 0\n",
    "    U_new[U_new>10] = 10\n",
    "    U_new = U_new.astype(int)\n",
    "    \n",
    "    return U_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rmseFunction(U_new,u,R):\n",
    "    \n",
    "    index = [u[R==1]!=0]\n",
    "    \n",
    "    \n",
    "    s = np.sqrt((((U_new[R==1][index] - u[R==1][index])**2).sum())/len(u[R==1][index]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90368162335382007"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmseFunction(U_new,u,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainSVD(u,R,t,alpha):\n",
    "    \n",
    "    X = np.random.random((u.shape[0],t))\n",
    "    theta = np.random.random((t,u.shape[1]))\n",
    "    \n",
    "    U, U_i_mean, U_j_mean = normalizeUtility(u.copy(),R)\n",
    "    \n",
    "    parameters = np.concatenate((np.ravel(X), np.ravel(theta)))\n",
    "    \n",
    "    fmin = minimize(fun=costFunction, x0=parameters, args=(U, R,t,alpha), \n",
    "                method='CG', jac=True, options={'maxiter': 100})\n",
    "    \n",
    "    U_new = prediction(fmin.x, U, t, U_i_mean, U_j_mean)\n",
    "    \n",
    "    return U_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_isbnDict, small_userDict = createSampleDict(new_isbnDict, new_userDict, 20, 20)\n",
    "\n",
    "index = sorted(small_userDict.keys())\n",
    "columns = sorted(small_isbnDict.keys())\n",
    "\n",
    "dict_row = {k:v for v,k in enumerate(index)}\n",
    "dict_col = {k:v for v,k in enumerate(columns)}\n",
    "\n",
    "#utility_DataFrame = pd.DataFrame(u, index = index, columns = columns)\n",
    "#R_DataFrame = pd.DataFrame(R, index = index, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(train_userDict,train_isbnDict,small_userDict,small_isbnDict,dict_row,dict_col,t=100,alpha=10):\n",
    "    \n",
    "    u,R,utility_DataFrame = computeMatrices(train_userDict,train_isbnDict,small_userDict,small_isbnDict, dict_row, dict_col)\n",
    "    U_new = mainSVD(u,R,t,alpha)\n",
    "    rmse = rmseFunction(U_new,u,R)\n",
    "    \n",
    "    return rmse, U_new, u, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgos/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/georgos/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "rmse_tot, U_new_tot, u_tot, R_tot = main(small_userDict,small_isbnDict,small_userDict,small_isbnDict,dict_row,dict_col,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgos/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/georgos/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/georgos/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 3521 but corresponding boolean dimension is 704\n",
      "  app.launch_new_instance()\n",
      "/home/georgos/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 3521 but corresponding boolean dimension is 704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "RMSE = []\n",
    "k = 5\n",
    "t = 100\n",
    "alpha = 1\n",
    "list_user = np.array(sorted(small_userDict.keys()))\n",
    "b = len(small_userDict)//k\n",
    "\n",
    "for iterator in range(k):\n",
    "    \n",
    "    train_userDict, train_isbnDict, test = selectSample(iterator,b,list_user,small_userDict, small_isbnDict)\n",
    "    \n",
    "    u, R, utility_DataFrame = computeMatrices(train_userDict,train_isbnDict,small_userDict,small_isbnDict, dict_row, dict_col)\n",
    "    U_new = mainSVD(u,R,t,alpha)\n",
    "    \n",
    "    R_f = reduced(test,dict_row,R_tot)\n",
    "    rmse_test = rmseFunction(U_new,u_tot,R_f)\n",
    "    print(\"Ok\")\n",
    "    \n",
    "    \n",
    "    RMSE.append(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3153658697085371"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def selectSample(i,b,list_user,small_userDict, small_isbnDict):\n",
    "\n",
    "    test_index = [k for k in range((i*b),(i+1)*b)]\n",
    "    train_index = [k for k in range(len(list_user)) if k not in test_index]\n",
    "\n",
    "    train = list_user[train_index]\n",
    "    test  = list_user[test_index]\n",
    "\n",
    "    train_userDict = {}\n",
    "    train_isbnDict = {isbn:{} for isbn in small_isbnDict}\n",
    "\n",
    "    for user in train:\n",
    "\n",
    "        train_userDict[user] = small_userDict[user]\n",
    "\n",
    "    for isbn in small_isbnDict:\n",
    "        for user in small_isbnDict[isbn]:\n",
    "\n",
    "            try:\n",
    "                train_userDict[user]\n",
    "                train_isbnDict[isbn][user] = small_isbnDict[isbn][user]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return train_userDict, train_isbnDict, test\n",
    "\n",
    "\n",
    "def reduced(test,dict_row,R):\n",
    "    \n",
    "    v = []\n",
    "    for row in test:\n",
    "        v.append(dict_row[row])\n",
    "\n",
    "    v = sorted(v)    \n",
    "    R_f = R[v,:]\n",
    "    \n",
    "    return R_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_index = np.random.choice(indices, size = b, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(list_user))]\n",
    "indices = np.array(indices)\n",
    "k = 5\n",
    "b = len(small_userDict)//k\n",
    "\n",
    "def selectSampleRandom(indices,b,list_user,small_userDict, small_isbnDict):\n",
    "    \n",
    "    test_index = np.random.choice(indices, size = b, replace=False)\n",
    "    \n",
    "    indices = np.delete(indices,test_index)\n",
    "    \n",
    "    train_index = [k for k in range(len(list_user)) if k not in test_index]\n",
    "    \n",
    "\n",
    "    train = list_user[train_index]\n",
    "    test  = list_user[test_index]\n",
    "\n",
    "    train_userDict = {}\n",
    "    train_isbnDict = {isbn:{} for isbn in small_isbnDict}\n",
    "\n",
    "    for user in train:\n",
    "\n",
    "        train_userDict[user] = small_userDict[user]\n",
    "\n",
    "    for isbn in small_isbnDict:\n",
    "        for user in small_isbnDict[isbn]:\n",
    "\n",
    "            try:\n",
    "                train_userDict[user]\n",
    "                train_isbnDict[isbn][user] = small_isbnDict[isbn][user]\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return train_userDict, train_isbnDict, test, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3521\n",
      "2817\n",
      "2265\n",
      "1806\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    train_userDict, train_isbnDict, test, indices = selectSampleRandom(indices,b,list_user,small_userDict, small_isbnDict)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
